{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to Week 6 Day 3!\n",
    "\n",
    "Let's experiment with a bunch more MCP Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, trace\n",
    "from agents.mcp import MCPServerStdio\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first type of MCP Server: runs locally, everything local\n",
    "\n",
    "Here's a really interesting one: a knowledge-graph based memory.\n",
    "\n",
    "It's a persistent memory store of entities, observations about them, and relationships between them.\n",
    "\n",
    "https://github.com/modelcontextprotocol/servers/tree/main/src/memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='create_entities', description='Create multiple new entities in the knowledge graph', inputSchema={'type': 'object', 'properties': {'entities': {'type': 'array', 'items': {'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'The name of the entity'}, 'entityType': {'type': 'string', 'description': 'The type of the entity'}, 'observations': {'type': 'array', 'items': {'type': 'string'}, 'description': 'An array of observation contents associated with the entity'}}, 'required': ['name', 'entityType', 'observations']}}}, 'required': ['entities']}, annotations=None),\n",
       " Tool(name='create_relations', description='Create multiple new relations between entities in the knowledge graph. Relations should be in active voice', inputSchema={'type': 'object', 'properties': {'relations': {'type': 'array', 'items': {'type': 'object', 'properties': {'from': {'type': 'string', 'description': 'The name of the entity where the relation starts'}, 'to': {'type': 'string', 'description': 'The name of the entity where the relation ends'}, 'relationType': {'type': 'string', 'description': 'The type of the relation'}}, 'required': ['from', 'to', 'relationType']}}}, 'required': ['relations']}, annotations=None),\n",
       " Tool(name='add_observations', description='Add new observations to existing entities in the knowledge graph', inputSchema={'type': 'object', 'properties': {'observations': {'type': 'array', 'items': {'type': 'object', 'properties': {'entityName': {'type': 'string', 'description': 'The name of the entity to add the observations to'}, 'contents': {'type': 'array', 'items': {'type': 'string'}, 'description': 'An array of observation contents to add'}}, 'required': ['entityName', 'contents']}}}, 'required': ['observations']}, annotations=None),\n",
       " Tool(name='delete_entities', description='Delete multiple entities and their associated relations from the knowledge graph', inputSchema={'type': 'object', 'properties': {'entityNames': {'type': 'array', 'items': {'type': 'string'}, 'description': 'An array of entity names to delete'}}, 'required': ['entityNames']}, annotations=None),\n",
       " Tool(name='delete_observations', description='Delete specific observations from entities in the knowledge graph', inputSchema={'type': 'object', 'properties': {'deletions': {'type': 'array', 'items': {'type': 'object', 'properties': {'entityName': {'type': 'string', 'description': 'The name of the entity containing the observations'}, 'observations': {'type': 'array', 'items': {'type': 'string'}, 'description': 'An array of observations to delete'}}, 'required': ['entityName', 'observations']}}}, 'required': ['deletions']}, annotations=None),\n",
       " Tool(name='delete_relations', description='Delete multiple relations from the knowledge graph', inputSchema={'type': 'object', 'properties': {'relations': {'type': 'array', 'items': {'type': 'object', 'properties': {'from': {'type': 'string', 'description': 'The name of the entity where the relation starts'}, 'to': {'type': 'string', 'description': 'The name of the entity where the relation ends'}, 'relationType': {'type': 'string', 'description': 'The type of the relation'}}, 'required': ['from', 'to', 'relationType']}, 'description': 'An array of relations to delete'}}, 'required': ['relations']}, annotations=None),\n",
       " Tool(name='read_graph', description='Read the entire knowledge graph', inputSchema={'type': 'object', 'properties': {}}, annotations=None),\n",
       " Tool(name='search_nodes', description='Search for nodes in the knowledge graph based on a query', inputSchema={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'The search query to match against entity names, types, and observation content'}}, 'required': ['query']}, annotations=None),\n",
       " Tool(name='open_nodes', description='Open specific nodes in the knowledge graph by their names', inputSchema={'type': 'object', 'properties': {'names': {'type': 'array', 'items': {'type': 'string'}, 'description': 'An array of entity names to retrieve'}}, 'required': ['names']}, annotations=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"command\": \"npx\",\"args\": [\"-y\", \"mcp-memory-libsql\"],\"env\": {\"LIBSQL_URL\": \"file:./memory/ed.db\"}}\n",
    "\n",
    "async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as server:\n",
    "    mcp_tools = await server.list_tools()\n",
    "\n",
    "mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "The below memory style is more stable and given in latest code:"
=======
    "instructions = \"You use your entity tools as a persistent memory to store and recall information about your conversations.\"\n",
    "request = \"My name's Ed. I'm an LLM engineer. I'm teaching a course about AI Agents, including the incredible MCP protocol. \\\n",
    "MCP is a protocol for connecting agents with tools, resources and prompt templates, and makes it easy to integrate AI agents with capabilities.\"\n",
    "model = \"gpt-4.1-mini\""
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='create_entities', description='Create new entities with observations and optional embeddings', inputSchema={'type': 'object', 'properties': {'entities': {'type': 'array', 'items': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'entityType': {'type': 'string'}, 'observations': {'type': 'array', 'items': {'type': 'string'}}, 'embedding': {'type': 'array', 'items': {'type': 'number'}, 'description': 'Optional vector embedding for similarity search'}}, 'required': ['name', 'entityType', 'observations']}}}, 'required': ['entities']}, annotations=None),\n",
       " Tool(name='search_nodes', description='Search for entities and their relations using text or vector similarity', inputSchema={'type': 'object', 'properties': {'query': {'oneOf': [{'type': 'string', 'description': 'Text search query'}, {'type': 'array', 'items': {'type': 'number'}, 'description': 'Vector for similarity search'}]}}, 'required': ['query']}, annotations=None),\n",
       " Tool(name='read_graph', description='Get recent entities and their relations', inputSchema={'type': 'object', 'properties': {}, 'required': []}, annotations=None),\n",
       " Tool(name='create_relations', description='Create relations between entities', inputSchema={'type': 'object', 'properties': {'relations': {'type': 'array', 'items': {'type': 'object', 'properties': {'source': {'type': 'string'}, 'target': {'type': 'string'}, 'type': {'type': 'string'}}, 'required': ['source', 'target', 'type']}}}, 'required': ['relations']}, annotations=None),\n",
       " Tool(name='delete_entity', description='Delete an entity and all its associated data (observations and relations)', inputSchema={'type': 'object', 'properties': {'name': {'type': 'string', 'description': 'Name of the entity to delete'}}, 'required': ['name']}, annotations=None),\n",
       " Tool(name='delete_relation', description='Delete a specific relation between entities', inputSchema={'type': 'object', 'properties': {'source': {'type': 'string', 'description': 'Source entity name'}, 'target': {'type': 'string', 'description': 'Target entity name'}, 'type': {'type': 'string', 'description': 'Type of relation'}}, 'required': ['source', 'target', 'type']}, annotations=None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parmas= {\n",
    "    \"command\": \"npx\",\n",
    "    \"args\": [\"-y\", \"mcp-memory-libsql\"],\n",
    "    \"env\": {\"LIBSQLURL\": \"file:./memory/aman.db\"}\n",
    "}\n",
    "\n",
    "async with MCPServerStdio(params=parmas, client_session_timeout_seconds=30) as server:\n",
    "    mcp_tools = await server.list_tools()\n",
    "mcp_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "instructions = \"You use your entity tools as a persistent memory to store and recall information about your conversations.\"\n",
    "request = f\"My name's Aman. I'm an LLM engineer. I'm teaching a course about AI Agents to ONGC academy, including the incredible MCP protocol. \\\n",
    "MCP is a protocol for connecting agents with tools, resources and prompt templates, and makes it easy to integrate AI agents with capabilities.\"\n",
    "model = \"gpt-4.1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Aman! I've stored the information about you, your role as an LLM engineer, your course on AI Agents at ONGC academy, and the MCP protocol that you include in the course. If you want, I can help you with generating course materials, explanations about MCP, or anything else related to your teaching. What would you like to do next?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async with MCPServerStdio(params=params) as mcp_server:\n",
=======
    "async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as mcp_server:\n",
>>>>>>> upstream/main
    "    agent = Agent(name=\"agent\", instructions=instructions, model=model, mcp_servers=[mcp_server])\n",
    "    with trace(\"conversation\"):\n",
    "        result = await Runner.run(agent, request)\n",
    "    display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I've stored the information about you and your course on AI Agents, including details about the MCP protocol. Here's what I have:\n",
       "\n",
       "### Entities Created:\n",
       "1. **Ed**\n",
       "   - Type: Person\n",
       "   - Observations:\n",
       "     - Ed is an LLM engineer.\n",
       "     - Ed is teaching a course about AI Agents on Udemy.\n",
       "     - Ed is teaching about the MCP protocol.\n",
       "\n",
       "2. **MCP Protocol**\n",
       "   - Type: Protocol\n",
       "   - Observations:\n",
       "     - MCP is a protocol for connecting agents with tools, resources, and prompt templates.\n",
       "     - MCP makes it easy to integrate AI agents with capabilities.\n",
       "\n",
       "3. **Udemy**\n",
       "   - Type: Platform\n",
       "   - Observations:\n",
       "     - Udemy is an online learning platform.\n",
       "\n",
       "4. **AI Agents**\n",
       "   - Type: Concept\n",
       "   - Observations:\n",
       "     - AI agents can interact with tools and resources to accomplish tasks.\n",
       "\n",
       "If there’s anything else you’d like to add or modify, let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "request = f\"My name's Ed. I'm an LLM engineer. I'm teaching a course about AI Agents at Udemy, including the incredible MCP protocol. \\\n",
    "MCP is a protocol for connecting agents with tools, resources and prompt templates, and makes it easy to integrate AI agents with capabilities.\"\n",
    "model = \"gpt-4o-mini\"\n",
    "async with MCPServerStdio(params=params) as mcp_server:\n",
    "    agent = Agent(name=\"agent\", instructions=instructions, model=model, mcp_servers=[mcp_server])\n",
    "    with trace(\"conversation\"):\n",
    "        result = await Runner.run(agent, request)\n",
    "    display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Aman and Ed are both LLM engineers and are teaching a course about AI Agents. However, there are no specific relations listed between Aman and Ed that define their connection beyond their professional roles in the same domain. Would you like to know more about either of them or the course they are involved in?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as mcp_server:\n",
    "    agent = Agent(name=\"agent\", instructions=instructions, model=model, mcp_servers=[mcp_server])\n",
    "    with trace(\"conversation\"):\n",
    "        result = await Runner.run(agent, \"How Aman and Ed are related?\")\n",
    "    display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the trace:\n",
    "\n",
    "https://platform.openai.com/traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 2nd type of MCP server - runs locally, calls a web service\n",
    "\n",
    "### Brave Search - apologies - this will need another API key! But it's free again.\n",
    "\n",
    "https://brave.com/search/api/\n",
    "\n",
    "Set up your account, and put your key in the .env under `BRAVE_API_KEY`"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\"BRAVE_API_KEY\": os.getenv(\"BRAVE_API_KEY\")}\n",
    "params = {\"command\": \"npx\", \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"], \"env\": env}\n",
    "\n",
    "async with MCPServerStdio(params=params) as server:\n",
<<<<<<< HEAD
    "    mcp_tools = await server.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='brave_web_search', description='Performs a web search using the Brave Search API, ideal for general queries, news, articles, and online content. Use this for broad information gathering, recent events, or when you need diverse web sources. Supports pagination, content filtering, and freshness controls. Maximum 20 results per request, with offset for pagination. ', inputSchema={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query (max 400 chars, 50 words)'}, 'count': {'type': 'number', 'description': 'Number of results (1-20, default 10)', 'default': 10}, 'offset': {'type': 'number', 'description': 'Pagination offset (max 9, default 0)', 'default': 0}}, 'required': ['query']}, annotations=None),\n",
       " Tool(name='brave_local_search', description=\"Searches for local businesses and places using Brave's Local Search API. Best for queries related to physical locations, businesses, restaurants, services, etc. Returns detailed information including:\\n- Business names and addresses\\n- Ratings and review counts\\n- Phone numbers and opening hours\\nUse this when the query implies 'near me' or mentions specific locations. Automatically falls back to web search if no local results are found.\", inputSchema={'type': 'object', 'properties': {'query': {'type': 'string', 'description': \"Local search query (e.g. 'pizza near Central Park')\"}, 'count': {'type': 'number', 'description': 'Number of results (1-20, default 5)', 'default': 5}}, 'required': ['query']}, annotations=None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
=======
    "    mcp_tools = await server.list_tools()\n",
    "\n",
>>>>>>> upstream/main
    "mcp_tools"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 7,
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are able to search the web for information and briefly summarize the takeaways.\"\n",
    "request = f\"Please research the latest news on Amazon stock price and briefly summarize its outlook. \\\n",
    "For context, the current date is {datetime.now().strftime('%Y-%m-%d')}\"\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As of mid-June 2025, Amazon's stock (NASDAQ: AMZN) is priced at approximately **$212.10**, with a recent peak reaching **$242.06** earlier in the year. Here are some key takeaways regarding its outlook:\n",
       "\n",
       "1. **Current Performance**: The stock has seen a modest increase, with revenue from Amazon Ads reporting a year-over-year growth of **19%** in Q1 2025. This growth is contributing to overall positive market sentiment.\n",
       "\n",
       "2. **Price Predictions**: Analysts have different forecasts for the stock's future. Most predict a price range between **$203.34** and **$236.87** for 2025, with a median one-year target of **$241.29**, suggesting a potential upside of about **17.29%**.\n",
       "\n",
       "3. **Market Sentiment**: The broader sentiment appears cautiously optimistic, with forecasts indicating possible increases in the stock price, especially moving into **2026**, where projections suggest it may climb to around **$244** by mid-year.\n",
       "\n",
       "4. **Volatility and Trends**: While the immediate outlook shows stability, the overall market conditions and economic factors, including inflation and consumer spending, could influence future performance. Analysts are keeping a close eye on these trends.\n",
       "\n",
       "In summary, Amazon's stock outlook seems encouraging, with analysts predicting potential growth, but investors should remain aware of market fluctuations that might impact performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async with MCPServerStdio(params=params, client_session_timeout_seconds=30) as mcp_server:\n",
    "    agent = Agent(name=\"agent\", instructions=instructions, model=model, mcp_servers=[mcp_server])\n",
    "    with trace(\"conversation\"):\n",
    "        result = await Runner.run(agent, request)\n",
    "    display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As usual, check out the trace:\n",
    "\n",
    "https://platform.openai.com/traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now the third type: running remotely\n",
    "\n",
    "It's actually really hard to find a \"remote MCP server\" aka \"hosted MCP server\" aka \"managed MCP server\".\n",
    "\n",
    "It's not a common model for using or sharing MCP servers, and there isn't a standard way to discover remote MCP servers.\n",
    "\n",
    "Anthropic lists some remote MCP servers, but these are for paid applications with business users:\n",
    "\n",
    "https://docs.anthropic.com/en/docs/agents-and-tools/remote-mcp-servers\n",
    "\n",
    "CloudFlare has tooling for you to create and deploy your own remote MCP servers, but this does not seem to be a common practice:\n",
    "\n",
<<<<<<< HEAD
    "https://www.alphavantage.co/\n",
    "\n",
    "Click \"Get Free API Key\"\n",
    "\n",
    "And enter that in your .env as `ALPHA_VANTAGE_API_KEY`\n",
    "\n",
    "You also need to create a Smithery account at:\n",
    "\n",
    "https://smithery.ai\n",
    "\n",
    "And create a new free API key, and enter that in your .env as `SMITHERY_API_KEY`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "\n",
    "alpha_vantage_api_key = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "smithery_api_key = os.getenv(\"SMITHERY_API_KEY\")\n",
    "\n",
    "config = {\"alphaVantageApiKey\": alpha_vantage_api_key}\n",
    "config_b64 = base64.b64encode(json.dumps(config).encode()).decode()\n",
    "url = f\"wss://server.smithery.ai/@qubaomingg/stock-analysis-mcp/ws?config={config_b64}&api_key={smithery_api_key}\"\n"
=======
    "https://developers.cloudflare.com/agents/guides/remote-mcp-server/\n"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "#### Difficulties with the OpenAI Agents SDK SSE Client\n",
    "\n",
    "This code should work (I believe) but it didn't!\n",
    "\n",
    "```\n",
    "params = {\"url\": url}\n",
    "async with MCPServerSse(params=params) as server:\n",
    "    mcp_tools = await server.list_tools()\n",
    "```\n",
    "\n",
    "So I wrote a quick client myself - see financial_datasets_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error initializing MCP server: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/amansharma/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3668, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/kv/fwt1l7t944ndvbwy1r6qftzr0000gn/T/ipykernel_91618/547573869.py\", line 2, in <module>\n",
      "  |     async with MCPServerSse(params=params) as server:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/amansharma/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/agents/mcp/server.py\", line 94, in __aenter__\n",
      "  |     await self.connect()\n",
      "  |   File \"/Users/amansharma/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/agents/mcp/server.py\", line 107, in connect\n",
      "  |     transport = await self.exit_stack.enter_async_context(self.create_streams())\n",
      "  |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/miniconda3/lib/python3.12/contextlib.py\", line 659, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/miniconda3/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/amansharma/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 43, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/amansharma/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/amansharma/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 53, in sse_client\n",
      "    |     event_source.response.raise_for_status()\n",
      "    |   File \"/Users/amansharma/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/httpx/_models.py\", line 829, in raise_for_status\n",
      "    |     raise HTTPStatusError(message, request=request, response=self)\n",
      "    | httpx.HTTPStatusError: Client error '400 Bad Request' for url 'wss://server.smithery.ai/@qubaomingg/stock-analysis-mcp/ws?config=eyJhbHBoYVZhbnRhZ2VBcGlLZXkiOiAiNEw1UTNaTFpEOFBFVFczRCJ9&api_key=d27e19c2-1bbe-4f15-875e-ba89671650c2'\n",
      "    | For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "params = {\"url\": url}\n",
    "async with MCPServerSse(params=params) as server:\n",
    "    mcp_tools = await server.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As of May 2025, this is hanging - hopefully it will start working again soon! Otherwise, please skip to the next section with the STOP sign..\n",
    "\n",
    "from alpha_client_streamable_http import get_stock_tools_openai\n",
    "openai_tools = await get_stock_tools_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FunctionTool(name='get-stock-data', description='get stock data', params_json_schema={'type': 'object', 'properties': {'symbol': {'type': 'string', 'description': 'Stock symbol (e.g., IBM, AAPL)'}}, 'required': ['symbol'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}, on_invoke_tool=<function get_stock_tools_openai.<locals>.<lambda> at 0x10f13fa60>, strict_json_schema=True),\n",
       " FunctionTool(name='get-stock-alerts', description='get stock alerts', params_json_schema={'type': 'object', 'properties': {'symbol': {'type': 'string', 'description': 'Stock symbol (e.g., IBM, AAPL)'}}, 'required': ['symbol'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}, on_invoke_tool=<function get_stock_tools_openai.<locals>.<lambda> at 0x10f13f880>, strict_json_schema=True),\n",
       " FunctionTool(name='get-daily-stock-data', description='get daily stock data', params_json_schema={'type': 'object', 'properties': {'symbol': {'type': 'string', 'description': 'Stock symbol (e.g., IBM, AAPL)'}}, 'required': ['symbol'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}, on_invoke_tool=<function get_stock_tools_openai.<locals>.<lambda> at 0x10f13f100>, strict_json_schema=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You can use tools to get stock prices.\"\n",
    "request = f\"Please let me know the share price of Amazon.\"\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling get-stock-data with {'symbol': 'AMZN'}\n",
      "Result: meta=None content=[TextContent(type='text', text='Stock data for AMZN (5min intervals):\\n\\n2025-06-05 19:55:00:\\n  Open: 208.2000\\n  High: 208.3000\\n  Low: 207.9200\\n  Close: 208.0100\\n  Volume: 6899\\n\\n2025-06-05 19:50:00:\\n  Open: 208.1500\\n  High: 208.2400\\n  Low: 208.0000\\n  Close: 208.1750\\n  Volume: 9081\\n\\n2025-06-05 19:45:00:\\n  Open: 207.9600\\n  High: 208.1200\\n  Low: 207.8500\\n  Close: 208.0600\\n  Volume: 7033\\n\\n2025-06-05 19:40:00:\\n  Open: 207.8400\\n  High: 208.0000\\n  Low: 207.8000\\n  Close: 207.9700\\n  Volume: 3441\\n\\n2025-06-05 19:35:00:\\n  Open: 207.8500\\n  High: 208.0300\\n  Low: 207.7000\\n  Close: 207.8400\\n  Volume: 12412\\n\\n2025-06-05 19:30:00:\\n  Open: 207.8700\\n  High: 207.9000\\n  Low: 207.7000\\n  Close: 207.8025\\n  Volume: 2682\\n\\n2025-06-05 19:25:00:\\n  Open: 207.8100\\n  High: 207.9600\\n  Low: 207.7000\\n  Close: 207.8000\\n  Volume: 5364\\n\\n2025-06-05 19:20:00:\\n  Open: 207.6900\\n  High: 207.8900\\n  Low: 207.6900\\n  Close: 207.7000\\n  Volume: 1443\\n\\n2025-06-05 19:15:00:\\n  Open: 207.8963\\n  High: 207.9000\\n  Low: 207.6700\\n  Close: 207.7900\\n  Volume: 4988\\n\\n2025-06-05 19:10:00:\\n  Open: 207.7500\\n  High: 207.9000\\n  Low: 207.6500\\n  Close: 207.8100\\n  Volume: 1332\\n\\n... and 90 more data points available.\\n', annotations=None)] isError=False\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The current share price of Amazon (AMZN) is approximately **$208.01**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = Agent(name=\"agent\", instructions=instructions, model=model, tools=openai_tools)\n",
    "with trace(\"conversation\"):\n",
    "    result = await Runner.run(agent, request)\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As usual, check out the trace:\n",
    "\n",
    "https://platform.openai.com/traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newly Added Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the third type: running remotely\n",
    "It's actually really hard to find a \"remote MCP server\" aka \"hosted MCP server\" aka \"managed MCP server\".\n",
    "\n",
    "It's not a common model for using or sharing MCP servers, and there isn't a standard way to discover remote MCP servers.\n",
    "\n",
    "Anthropic lists some remote MCP servers, but these are for paid applications with business users:\n",
    "\n",
    "https://docs.anthropic.com/en/docs/agents-and-tools/remote-mcp-servers\n",
    "\n",
    "CloudFlare has tooling for you to create and deploy your own remote MCP servers, but this does not seem to be a common practice:\n",
    "\n",
    "https://developers.cloudflare.com/agents/guides/remote-mcp-server/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New section - Polygon.io MCP Server"
=======
    "# And back to the 2nd type: the Polygon.io MCP Server"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">PLEASE READ!!-</h2>\n",
    "            <span style=\"color:#ff7800;\">This service for financial market data has both a FREE plan and a PAID plan, and we can use either depending on your appetite.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW SECTION: Introducing polygon.io\n",
    "\n",
    "Polygon.io is a hugely popular financial data provider. It has a free plan and a paid plan. And it also has an MCP Server!\n",
    "\n",
    "First, read up on polygon.io on their excellent website, including looking at their pricing:\n",
    "\n",
    "https://polygon.io\n",
    "\n",
    "### Polygon.io Part 1: Polygon.io free service (the paid will be totally optional, of course!)\n",
    "\n",
    "1. Please sign up for polygon.io (top right)  \n",
    "2. Once signed in, please select \"Keys\" in the left hand navigation\n",
    "3. Press the blue \"New Key\" button\n",
    "4. Copy the key name\n",
    "5. Edit your .env file and add the row:\n",
    "\n",
    "`POLYGON_API_KEY=xxxx`"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> upstream/main
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "polygon_api_key = os.getenv(\"POLYGON_API_KEY\")\n",
    "if not polygon_api_key:\n",
    "    print(\"POLYGON_API_KEY is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreviousCloseAgg(ticker='AAPL', close=196.45, high=200.37, low=195.7, open=199.73, timestamp=1749844800000, volume=51447349.0, vwap=197.0361)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from polygon import RESTClient\n",
    "client = RESTClient(polygon_api_key)\n",
    "client.get_previous_close_agg(\"AAPL\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = client.get_previous_close_agg(\"AAPL\")[0]\n",
    "last_close = datetime.fromtimestamp(probe.timestamp/1000).date()\n",
    "\n",
    "results = client.get_grouped_daily_aggs(last_close, adjusted=True, include_otc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2025, 6, 14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapped into a python module that caches end of day prices\n",
    "\n",
    "I've made a python module `market.py` that uses this API to look up share prices.\n",
    "\n",
    "But the free API is quite heavily rate limited - so I've been a bit sneaky; when you ask for a share price, this function retrieves the entire end-of-day equity market, and caches it in our database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from market import is_market_open\n",
    "is_market_open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from market import get_share_price, get_market_for_prior_date\n",
    "get_market_for_prior_date.cache_clear()\n",
    "get_share_price(\"AAPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no rate limiting concerns!\n",
    "\n",
    "for i in range(1000):\n",
    "    get_share_price(\"AAPL\")\n",
    "get_share_price(\"AAPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And I've made this into an MCP Server\n",
    "\n",
    "Just as we did with accounts.py; see `market_server.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='lookup_share_price', description='This tool provides the current price of the given stock symbol.\\n\\n    Args:\\n        symbol: the symbol of the stock\\n    ', inputSchema={'properties': {'symbol': {'title': 'Symbol', 'type': 'string'}}, 'required': ['symbol'], 'title': 'lookup_share_priceArguments', 'type': 'object'}, annotations=None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"command\": \"uv\", \"args\": [\"run\", \"market_server.py\"]}\n",
    "async with MCPServerStdio(params=params) as server:\n",
    "    mcp_tools = await server.list_tools()\n",
    "mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it out!\n",
    "\n",
    "Hopefully gpt-4o-mini is smart enough to know that the symbol for Apple is AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It seems there was an issue retrieving the share price for Apple (AAPL), as it returned a value of 0.0. You might want to check a reliable stock market platform for the latest price."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instructions = \"You answer questions about the stock market.\"\n",
    "request = \"What's the share price of Apple?\"\n",
    "model = \"gpt-4.1-mini\"\n",
    "\n",
    "async with MCPServerStdio(params=params) as mcp_server:\n",
    "    agent = Agent(name=\"agent\", instructions=instructions, model=model, mcp_servers=[mcp_server])\n",
    "    with trace(\"conversation\"):\n",
    "        result = await Runner.run(agent, request)\n",
    "    display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygon.io Part 2: Paid Plan - Totally Optional!\n",
    "\n",
    "If you are interested, you can subscribe to the monthly plan to get more up to date market data, and unlimited API calls.\n",
    "\n",
    "If you do wish to do this, then it also makes sense to use the full MCP server that Polygon.io has released, to take advantage of all their functionality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error initializing MCP server: Timed out while waiting for response to ClientRequest. Waited 90.0 seconds.\n",
      "Error cleaning up server: unhandled errors in a TaskGroup (1 sub-exception)\n"
     ]
    },
    {
     "ename": "McpError",
     "evalue": "Timed out while waiting for response to ClientRequest. Waited 90.0 seconds.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWouldBlock\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/anyio/streams/memory.py:111\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreceive_nowait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m WouldBlock:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# Add ourselves in the queue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/anyio/streams/memory.py:106\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive_nowait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m WouldBlock\n",
      "\u001b[31mWouldBlock\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/anyio/_core/_tasks.py:115\u001b[39m, in \u001b[36mfail_after\u001b[39m\u001b[34m(delay, shield)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_async_backend().create_cancel_scope(\n\u001b[32m    113\u001b[39m     deadline=deadline, shield=shield\n\u001b[32m    114\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m cancel_scope:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m cancel_scope\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancel_scope.cancelled_caught \u001b[38;5;129;01mand\u001b[39;00m current_time() >= cancel_scope.deadline:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/mcp/shared/session.py:254\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type, request_read_timeout_seconds)\u001b[39m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m         response_or_error = \u001b[38;5;28;01mawait\u001b[39;00m response_stream_reader.receive()\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/anyio/streams/memory.py:119\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m receive_event.wait()\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1774\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1773\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1774\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event.wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mCancelledError\u001b[39m: Cancelled by cancel scope 10ba9b710",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/mcp/shared/session.py:253\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type, request_read_timeout_seconds)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m    254\u001b[39m         response_or_error = \u001b[38;5;28;01mawait\u001b[39;00m response_stream_reader.receive()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/anyio/_core/_tasks.py:118\u001b[39m, in \u001b[36mfail_after\u001b[39m\u001b[34m(delay, shield)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancel_scope.cancelled_caught \u001b[38;5;129;01mand\u001b[39;00m current_time() >= cancel_scope.deadline:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mMcpError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m params = {\u001b[33m\"\u001b[39m\u001b[33mcommand\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muvx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      2\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33m--from\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgit+https://github.com/polygon-io/mcp_polygon.git\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmcp_polygon\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      3\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33menv\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mPOLYGON_API_KEY\u001b[39m\u001b[33m\"\u001b[39m: os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPOLYGON_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)}\n\u001b[32m      4\u001b[39m           }\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m MCPServerStdio(params=params, client_session_timeout_seconds=\u001b[32m90\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m server:\n\u001b[32m      6\u001b[39m     mcp_tools = \u001b[38;5;28;01mawait\u001b[39;00m server.list_tools()\n\u001b[32m      7\u001b[39m mcp_tools\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/agents/mcp/server.py:94\u001b[39m, in \u001b[36m_MCPServerWithClientSession.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.connect()\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/agents/mcp/server.py:118\u001b[39m, in \u001b[36m_MCPServerWithClientSession.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     read, write = transport\n\u001b[32m    109\u001b[39m     session = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exit_stack.enter_async_context(\n\u001b[32m    110\u001b[39m         ClientSession(\n\u001b[32m    111\u001b[39m             read,\n\u001b[32m   (...)\u001b[39m\u001b[32m    116\u001b[39m         )\n\u001b[32m    117\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m session.initialize()\n\u001b[32m    119\u001b[39m     \u001b[38;5;28mself\u001b[39m.session = session\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/mcp/client/session.py:126\u001b[39m, in \u001b[36mClientSession.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m sampling = types.SamplingCapability()\n\u001b[32m    119\u001b[39m roots = types.RootsCapability(\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# TODO: Should this be based on whether we\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# _will_ send notifications, or only whether\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# they're supported?\u001b[39;00m\n\u001b[32m    123\u001b[39m     listChanged=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    124\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send_request(\n\u001b[32m    127\u001b[39m     types.ClientRequest(\n\u001b[32m    128\u001b[39m         types.InitializeRequest(\n\u001b[32m    129\u001b[39m             method=\u001b[33m\"\u001b[39m\u001b[33minitialize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m             params=types.InitializeRequestParams(\n\u001b[32m    131\u001b[39m                 protocolVersion=types.LATEST_PROTOCOL_VERSION,\n\u001b[32m    132\u001b[39m                 capabilities=types.ClientCapabilities(\n\u001b[32m    133\u001b[39m                     sampling=sampling,\n\u001b[32m    134\u001b[39m                     experimental=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    135\u001b[39m                     roots=roots,\n\u001b[32m    136\u001b[39m                 ),\n\u001b[32m    137\u001b[39m                 clientInfo=\u001b[38;5;28mself\u001b[39m._client_info,\n\u001b[32m    138\u001b[39m             ),\n\u001b[32m    139\u001b[39m         )\n\u001b[32m    140\u001b[39m     ),\n\u001b[32m    141\u001b[39m     types.InitializeResult,\n\u001b[32m    142\u001b[39m )\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.protocolVersion \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_PROTOCOL_VERSIONS:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsupported protocol version from the server: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.protocolVersion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/mcp/shared/session.py:256\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type, request_read_timeout_seconds)\u001b[39m\n\u001b[32m    254\u001b[39m         response_or_error = \u001b[38;5;28;01mawait\u001b[39;00m response_stream_reader.receive()\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(\n\u001b[32m    257\u001b[39m         ErrorData(\n\u001b[32m    258\u001b[39m             code=httpx.codes.REQUEST_TIMEOUT,\n\u001b[32m    259\u001b[39m             message=(\n\u001b[32m    260\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimed out while waiting for response to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    261\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Waited \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    262\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    263\u001b[39m             ),\n\u001b[32m    264\u001b[39m         )\n\u001b[32m    265\u001b[39m     )\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response_or_error, JSONRPCError):\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(response_or_error.error)\n",
      "\u001b[31mMcpError\u001b[39m: Timed out while waiting for response to ClientRequest. Waited 90.0 seconds."
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\"command\": \"uvx\",\n",
    "          \"args\": [\"--from\", \"git+https://github.com/polygon-io/mcp_polygon@master\", \"mcp_polygon\"],\n",
    "          \"env\": {\"POLYGON_API_KEY\": os.getenv(\"POLYGON_API_KEY\")}\n",
    "          }\n",
    "async with MCPServerStdio(params=params, client_session_timeout_seconds=90) as server:\n",
    "    mcp_tools = await server.list_tools()\n",
    "mcp_tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow that's a lot of tools!\n",
    "\n",
    "Let's try them out - hopefully the sheer number of tools doesn't overwhelm gpt-4o-mini!\n",
    "\n",
    "With the $29 monthly plan, we don't have access to some of the APIs, so I've needed to specify which APIs can be called.\n",
    "\n",
    "If you've splashed out on a bigger plan, feel free to remove my extra constraint.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You answer questions about the stock market.\"\n",
    "request = \"What's the share price of Apple? Use your get_snapshot_ticker tool to get the latest price.\"\n",
    "model = \"gpt-4.1-mini\"\n",
    "\n",
    "async with MCPServerStdio(params=params) as mcp_server:\n",
    "    agent = Agent(name=\"agent\", instructions=instructions, model=model, mcp_servers=[mcp_server])\n",
    "    with trace(\"conversation\"):\n",
    "        result = await Runner.run(agent, request)\n",
    "    display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your .env file\n",
    "\n",
    "If you do decide to have a paid plan, please add this to your .env file to indicate:\n",
    "\n",
    "`POLYGON_PLAN=paid`\n",
    "\n",
    "And if you decide to go all the way for the realtime API, then please do:\n",
    "\n",
    "`POLYGON_PLAN=realtime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "polygon_plan = os.getenv(\"POLYGON_PLAN\")\n",
    "is_paid_polygon = polygon_plan == \"paid\"\n",
    "is_realtime_polygon = polygon_plan == \"realtime\"\n",
    "\n",
    "if is_paid_polygon:\n",
    "    print(\"You've chosen to subscribe to the paid Polygon plan, so the code will look at prices on a 15 min delay\")\n",
    "elif is_realtime_polygon:\n",
    "    print(\"Wowzer - you've chosen to subscribe to the realtime Polygon plan, so the code will look at realtime prices\")\n",
    "else:\n",
    "    print(\"According to your .env file, you've chosen to subscribe to the free Polygon plan, so the code will look at EOD prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And that's it for today!\n",
    "\n",
    "I've removed the part of this lab that uses the \"Financial Datasets\" mcp server, because it's inferior - more expensive with fewer APIs.\n",
    "\n",
    "And this way we get to use the same provider for Free and Paid APIs.\n",
    "\n",
    "But if you want to see the code, just look in the git history for a prior version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercises</h2>\n",
    "            <span style=\"color:#ff7800;\">Explore MCP server marketplaces and integrate your own, using all 3 approaches.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
