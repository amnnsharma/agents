{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 Day 4\n",
    "\n",
    "AutoGen Core - Distributed\n",
    "\n",
    "I'm only going to give a Teaser of this!!\n",
    "\n",
    "Partly because I'm unsure how relevant it is to you. If you'd like me to add more content for this, please do let me know.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "ALL_IN_ONE_WORKER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with our Message class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now - a host for our distributed runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntimeHost\n",
    "\n",
    "host = GrpcWorkerAgentRuntimeHost(address=\"localhost:50051\")\n",
    "host.start() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's reintroduce a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "serper = GoogleSerperAPIWrapper()\n",
    "langchain_serper =Tool(name=\"internet_search\", func=serper.run, description=\"Useful for when you need to search the internet\")\n",
    "autogen_serper = LangChainToolAdapter(langchain_serper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction1 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons in favor of choosing AutoGen; the pros of AutoGen.\"\n",
    "\n",
    "instruction2 = \"To help with a decision on whether to use AutoGen in a new AI Agent project, \\\n",
    "please research and briefly respond with reasons against choosing AutoGen; the cons of Autogen.\"\n",
    "\n",
    "judge = \"You must make a decision on whether to use AutoGen for a project. \\\n",
    "Your research team has come up with the following reasons for and against. \\\n",
    "Based purely on the research from your team, please respond with your decision and brief rationale.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And make some Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client, tools=[autogen_serper], reflect_on_tool_use=True)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "class Judge(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "        \n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        message1 = Message(content=instruction1)\n",
    "        message2 = Message(content=instruction2)\n",
    "        inner_1 = AgentId(\"player1\", \"default\")\n",
    "        inner_2 = AgentId(\"player2\", \"default\")\n",
    "        response1 = await self.send_message(message1, inner_1)\n",
    "        response2 = await self.send_message(message2, inner_2)\n",
    "        result = f\"## Pros of AutoGen:\\n{response1.content}\\n\\n## Cons of AutoGen:\\n{response2.content}\\n\\n\"\n",
    "        judgement = f\"{judge}\\n{result}Respond with your decision and brief explanation\"\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([message], ctx.cancellation_token)\n",
    "        return Message(content=result + \"\\n\\n## Decision:\\n\\n\" + response.chat_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AioRpcError",
     "evalue": "<AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50052: Failed to connect to remote host: connect: Connection refused (61)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-05-30T16:40:47.591912+05:30\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50052: Failed to connect to remote host: connect: Connection refused (61)\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAioRpcError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Player1Agent.register(worker1, \u001b[33m\"\u001b[39m\u001b[33mplayer1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: Player1Agent(\u001b[33m\"\u001b[39m\u001b[33mplayer1\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     20\u001b[39m worker2 = GrpcWorkerAgentRuntime(host_address=\u001b[33m\"\u001b[39m\u001b[33mlocalhost:50052\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m worker2.start()\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Player2Agent.register(worker2, \u001b[33m\"\u001b[39m\u001b[33mplayer2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: Player2Agent(\u001b[33m\"\u001b[39m\u001b[33mplayer2\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     24\u001b[39m worker = GrpcWorkerAgentRuntime(host_address=\u001b[33m\"\u001b[39m\u001b[33mlocalhost:50053\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/autogen_ext/runtimes/grpc/_worker_runtime.py:265\u001b[39m, in \u001b[36mGrpcWorkerAgentRuntime.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRuntime is already running.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    264\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnecting to host: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._host_address\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28mself\u001b[39m._host_connection = \u001b[38;5;28;01mawait\u001b[39;00m HostConnection.from_host_address(\n\u001b[32m    266\u001b[39m     \u001b[38;5;28mself\u001b[39m._host_address, extra_grpc_config=\u001b[38;5;28mself\u001b[39m._extra_grpc_config\n\u001b[32m    267\u001b[39m )\n\u001b[32m    268\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mConnection established\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/autogen_ext/runtimes/grpc/_worker_runtime.py:151\u001b[39m, in \u001b[36mHostConnection.from_host_address\u001b[39m\u001b[34m(cls, host_address, extra_grpc_config)\u001b[39m\n\u001b[32m    148\u001b[39m stub: AgentRpcAsyncStub = agent_worker_pb2_grpc.AgentRpcStub(channel)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    149\u001b[39m instance = \u001b[38;5;28mcls\u001b[39m(channel, stub)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m instance._connection_task = \u001b[38;5;28;01mawait\u001b[39;00m instance._connect(\n\u001b[32m    152\u001b[39m     stub, instance._send_queue, instance._recv_queue, instance._client_id\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/autogen_ext/runtimes/grpc/_worker_runtime.py:177\u001b[39m, in \u001b[36mHostConnection._connect\u001b[39m\u001b[34m(stub, send_queue, receive_queue, client_id)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# TODO: where do exceptions from reading the iterable go? How do we recover from those?\u001b[39;00m\n\u001b[32m    173\u001b[39m stream: StreamStreamCall[agent_worker_pb2.Message, agent_worker_pb2.Message] = stub.OpenChannel(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    174\u001b[39m     QueueAsyncIterable(send_queue), metadata=[(\u001b[33m\"\u001b[39m\u001b[33mclient-id\u001b[39m\u001b[33m\"\u001b[39m, client_id)]\n\u001b[32m    175\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m stream.wait_for_connection()\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_loop\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/grpc/aio/_call.py:539\u001b[39m, in \u001b[36m_StreamRequestMixin.wait_for_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata_sent.wait()\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML/LLM Engineering/agents/.venv/lib/python3.12/site-packages/grpc/aio/_call.py:272\u001b[39m, in \u001b[36mCall._raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    270\u001b[39m code = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.code()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m code != grpc.StatusCode.OK:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _create_rpc_error(\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.initial_metadata(), \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cython_call.status()\n\u001b[32m    274\u001b[39m     )\n",
      "\u001b[31mAioRpcError\u001b[39m: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50052: Failed to connect to remote host: connect: Connection refused (61)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {created_time:\"2025-05-30T16:40:47.591912+05:30\", grpc_status:14, grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:50052: Failed to connect to remote host: connect: Connection refused (61)\"}\"\n>"
     ]
    }
   ],
   "source": [
    "from autogen_ext.runtimes.grpc import GrpcWorkerAgentRuntime\n",
    "\n",
    "if not ALL_IN_ONE_WORKER:\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "\n",
    "    await Player1Agent.register(worker, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "    await Player2Agent.register(worker, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
    "\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n",
    "\n",
    "else:\n",
    "\n",
    "    worker1 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker1.start()\n",
    "    await Player1Agent.register(worker1, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "\n",
    "    worker2 = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker2.start()\n",
    "    await Player2Agent.register(worker2, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "\n",
    "    worker = GrpcWorkerAgentRuntime(host_address=\"localhost:50051\")\n",
    "    await worker.start()\n",
    "    await Judge.register(worker, \"judge\", lambda: Judge(\"judge\"))\n",
    "    agent_id = AgentId(\"judge\", \"default\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await worker.send_message(Message(content=\"Go!\"), agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Pros of AutoGen:\n",
       "Here are some reasons in favor of using AutoGen in an AI Agent project:\n",
       "\n",
       "1. **Improved Efficiency**: AutoGen utilizes asynchronous messaging and an event-driven architecture, allowing for faster and more efficient communication among agents. This can significantly enhance the responsiveness of the AI system.\n",
       "\n",
       "2. **Scalability**: The modular and extensible nature of AutoGen facilitates the creation of scalable systems. This means that as your project grows, it can easily adapt to increased demand or expand its capabilities without substantial rework.\n",
       "\n",
       "3. **Customization**: With its flexible architecture, AutoGen allows for the development of tailored solutions to meet specific project needs, providing more control over the AI agent's behavior and functionality.\n",
       "\n",
       "4. **Ease of Integration**: AutoGen can often be integrated with various tools and platforms, making it easier to incorporate into existing workflows and systems.\n",
       "\n",
       "5. **Community and Support**: Depending on the ecosystem, AutoGen might have an active community, providing resources, documentation, and support, which can be beneficial for troubleshooting and development.\n",
       "\n",
       "These advantages make AutoGen a strong candidate for projects requiring robust, efficient, and scalable AI agents. \n",
       "\n",
       "TERMINATE\n",
       "\n",
       "## Cons of AutoGen:\n",
       "Here are some cons of using AutoGen for an AI Agent project:\n",
       "\n",
       "1. **Dependency on Input Quality**: The quality of content generated is highly contingent on the quality of input data and parameters provided. This necessitates careful selection and validation of inputs to achieve desirable outcomes.\n",
       "\n",
       "2. **Limited Customization**: AutoGen may have limitations in tailoring models or outputs to specific needs, which could hinder the flexibility required for specialized applications.\n",
       "\n",
       "3. **Complexity in Management**: Implementing and managing AutoGen might require additional expertise and resources, potentially complicating project workflows.\n",
       "\n",
       "4. **Cost Implications**: Depending on the pricing model, using AutoGen could lead to increased operational costs, especially for large-scale applications.\n",
       "\n",
       "5. **Performance Variability**: The performance can be inconsistent, influenced by various factors such as model updates and changes in underlying algorithms.\n",
       "\n",
       "6. **Integration Challenges**: Integrating AutoGen with existing systems and workflows might introduce challenges, requiring significant development effort.\n",
       "\n",
       "7. **Risk of Overfitting**: There is a risk that AutoGen-generated models may overfit to specific datasets, leading to reduced generalizability in real-world scenarios.\n",
       "\n",
       "These considerations should be weighed against any potential benefits when deciding whether to use AutoGen in your project. \n",
       "\n",
       "TERMINATE\n",
       "\n",
       "\n",
       "\n",
       "## Decision:\n",
       "\n",
       "After assessing both the pros and cons of using AutoGen for the AI Agent project, I have decided to utilize AutoGen.\n",
       "\n",
       "**Rationale**: The advantages of improved efficiency, scalability, and customization present compelling reasons to adopt AutoGen, particularly given the project's potential growth and need for tailored solutions. The community support also enhances the likelihood of successful implementation and troubleshooting. While there are valid concerns regarding input quality, management complexity, and cost, the benefits of enhanced responsiveness and the ability to scale outweigh these drawbacks, especially in a dynamic project environment.\n",
       "\n",
       "TERMINATE"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "await worker.stop()\n",
    "if not ALL_IN_ONE_WORKER:\n",
    "    await worker1.stop()\n",
    "    await worker2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "await host.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
